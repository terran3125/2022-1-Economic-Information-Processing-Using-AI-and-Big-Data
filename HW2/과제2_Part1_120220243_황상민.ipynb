{"cells":[{"cell_type":"markdown","source":["# 과제2_Part1_120220243_황상민"],"metadata":{"id":"n9B7PoTGRMmz"},"id":"n9B7PoTGRMmz"},{"cell_type":"markdown","source":["# ❗ Caution ❗\n","\n","이 코드는 특정 웹페이지에 대한 크롤링을 포함하고 있습니다.<br>\n","작성자는 크롤러에 적절한 time.sleep을 적용하지 않고 크롤러를 작성하여 쿠팡으로부터 한번에 크롤링이 되지 않도록 IP를 차단당하였습니다.<br>\n","그래서 혹시 교수님이나 조교님, 혹은 다른 학생분들께서 이 코드를 replicate하고자 하신다면, 코드 내에 충분한 수치로 부여된 time.sleep()을 임의로 줄이지 마시길 바랍니다.<br>\n","그러지 않을 경우에 추후 본인의 IP로 쿠팡 웹페이지를 크롤링하는데 어려움이 있을 수 있습니다. 그리고 Par1 크롤러에 포함된 request library 때문에 google colab 환경에서는 실행하기 어려우므로 local 환경에서 실행해야 합니다.<br>\n","Part1에는 데이터의 수집과정이 담겨있고, 과제의 결과만 확인하고 싶으시다면 Part2 코드를 참고하시길 바랍니다.<br><br>"],"metadata":{"id":"kuEuqQs6JE3W"},"id":"kuEuqQs6JE3W"},{"cell_type":"markdown","id":"8e07b591","metadata":{"id":"8e07b591"},"source":["# ❓ Question ❓\n","# 상품의 이미지와 e커머스 플랫폼 검색결과 페이지 번호와의 상관관계\n","### 쿠팡에서 세제 키워드를 중심으로"]},{"cell_type":"markdown","id":"dd4074f0","metadata":{"id":"dd4074f0"},"source":["### Motivation<br>\n","요즘 신제품을 출시하면 e커머스에 제품을 등록하는 것은 불가피하다. e커머스  플랫폼에서 사용자는 필터기능보다는 검색기능을 주로 활용한다. 검색은 키워드를 통해 이뤄지는데, 노출되는 수많은 제품 리스트는 한 화면에 표시되기에는 제한된다. 그래서 많은 e커머스 플랫폼은 여러 페이지에 제품을 노출한다. 상위 페이지에 노출될수록 소비자에게 많이 노출되어 매출을 높일 수 있다. 그러나 수많은 판매자의 수많은 제품을 e커머스 플랫폼에서 어떻게 배치할까?<br>\n","제품포장에서 시각이미지는 언어적이거나 비언어적으로 소비자의 이성적 또는 감성적으로 구매동기를 유발하고 촉진시키는 핵심적인 요소이자 상품의 경쟁력 강화를 위한 마케팅 커뮤니케이션 수단이 되고 있다.¹ 그래서 이번 과제에서는 e커머스 플랫폼 웹페이지에서 소비자가 키워드로 제품을 검색했을 때, 소비자에게 노출되는 상품의 이미지가 상품이 노출되는 페이지의 번호와 얼마나 강한 상관관계가 있는지 확인하고자 한다. 이를 실험하기 위해 쿠팡에서 주요 생필품 중 하나인 세제를 검색했을 때, 각 세제 상품이 노출되는 페이지 번호를 예측하는 CNN모델을 구축한다. 이를 통해 상품의 이미지와 페이지 번호 간의 상관관계 추정하고, 신제품의 이미지로 온라인 쇼핑몰에 진입했을 때 위치할 페이지 번호를 예측하고자 한다.\n",">¹김경선 ( Kim Kyung-sun ),and 강혜숙 ( Kang Hye-sook ). \"제품포장의 시각이미지에서 인지반응과 감성반응이 제품태도와 구매의도에 미치는 영향.\" 한국디자인포럼 28.- (2010): 197-210.\n","<br><br>\n"]},{"cell_type":"markdown","id":"1aacae04","metadata":{"id":"1aacae04"},"source":["### Experiment Environment<br>\n","쿠팡은 모바일 애플리케이션과 웹페이지로 플랫폼을 운영하고 있다. 이번 과제에서는 웹페이지의 검색결과를 중심으로 노출되는 페이지 번호를 예측하고자 한다. 검색결과 노출은 '쿠팡 랭킹순'이라는 기본 기준을 적용하여 노출되며 페이지당 36개의 상품이 노출된다. 쿠팡 랭킹순은 판매 실적, 사용자 선호도, 상품 정보 충실도 및 검색 정확도 등을 종합적으로 고려한 순위다. 즉, 이번 과제에서는 쿠팡에서 세제를 검색했을 때, 쿠팡 랭킹순으로 노출될 때의 페이지 번호를 이미지만으로 예측하는 실험을 수행한다.<br>\n","과제의 코드는 크게 두 파트로 나뉜다. 첫번째 파트는 크롤링을 통해 세제 상품정보 및 이미지를 수집하는 코드이고, 두번째 파트는 수집된 데이터를 이용해 예측을 수행하는 코드다. 첫번째 파트인 크롤링 코드는 Google Colab에서 실행할시, request library가 올바르게 작동하지 않아서 local 환경에서 replicator가 지정한 작업 디렉토리에서 수집된 데이터를 저장해야 한다. 두번째 파트는 작성자의 dropbox link를 통해 제공되는 미리 수집된 데이터를 사용하여 모델링 과정을 Google Colab 환경에서도 수행해볼 수 있다.\n","<br><br>"]},{"cell_type":"markdown","id":"19245977","metadata":{"id":"19245977"},"source":["### Issue & Applicable Solution\n","\n","이번 과제에서 효용대비 비용문제를 고려하여 해결하지 못했지만 솔루션이 존재하는 이슈가 있다.\n","\n","1) 같은 이미지를 사용하는 상품을 다른 판매자가 올린 경우<br>\n","-> 이미지 유사도를 검사하며 100% 일치하는 이미지의 경우, 상위 페이지에 위치한 이미지 삭제<br>\n","-> 새로 입점하는 상품은 하위 페이지에 위치할 가능성이 높기 때문에 상위 페이지 이미지를 삭제<br><br>\n","2) 같은 페이지 내에서도 상, 하단 순위 부여 부재<br>\n","-> 쿠팡 웹페이지 html tag 활용하여 Xpage의 Y번째 행에 위치한 상품의 페이지 번호를 X.(Y-1)로 부여(한 페이지에 10행의 이미지로 구성)<br>\n","-> e.g. 5페이지의 7번째 행에 위치한 상품은 5.6페이지로 페이지 번호 부여\n","<br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","id":"01d0bf2b","metadata":{"id":"01d0bf2b"},"source":["### Expandability<br>\n","이번 과제의 주제를 추후 팀 프로젝트 범위로 확장할 수 있는 방향을 제시한다.<br>\n","1) 여러 품목의 이미지와 페이지 번호 상관관계 비교<br>\n","여러 품목에서 모델의 페이지 번호 예측 성능을 비교함으로써 상품의 이미자와 페이지 번호의 상관관계가 강한 품목과 약한 품목을 추려낸다. 이를 바탕으로 e커머스 플랫폼은 페이지 노출 방식을 수정할 수 있다. 그리고 입점 업체는 상품 이미지와 페이지 번호의 상관관계에 따라 노출되는 상품의 어떤 정보를 수정해야 상위 페이지에 노출되는지 판단할 수 있다.<br>\n","\n","2) 특정 품목에 대한 여러 e커머스 플랫폼 페이지 번호 부여경향 비교<br>\n","특정 품목을 기준으로 서로 다른 e커머스 플랫폼에서 상품의 이미지와 페이지 번호의 상관관계를 파악한다. 이 경우에는 각 e커머스 플랫폼마다 다른 웹페이지 구조를 갖고있기 때문에 각각의 크롤러를 생성해야 할 것이다. 그리고 한 페이지에 배치된 상품 개수 및 키워드 당 최대 페이지 개수 등과 같은 여러 요소를 고려하여 데이터 수집이 이루어져야 한다.\n","<br><br>"]},{"cell_type":"markdown","source":["# 1. Import libraries"],"metadata":{"id":"5Y6_jAoaF7K7"},"id":"5Y6_jAoaF7K7"},{"cell_type":"code","execution_count":null,"id":"f29cc024","metadata":{"id":"f29cc024"},"outputs":[],"source":["import os\n","import requests\n","from bs4 import BeautifulSoup\n","import urllib.request\n","import pandas as pd\n","import time\n","import math\n","\n","#change path\n","#os.chdir('C:/Users/sangmin/OneDrive - 아주대학교/onedrive_pypr/경제정보처리/과제2')\n","\n","#os.chdir('INPUT YOUR LOCAL PATH') # request library의 문제로 인하여 코랩에서 part1 실행이 어려움"]},{"cell_type":"markdown","source":["# 2. Define Functions"],"metadata":{"id":"5jMyZqzJM4Qp"},"id":"5jMyZqzJM4Qp"},{"cell_type":"markdown","source":["## 2.1. 검색된 키워드 마지막 페이지 번호 추출"],"metadata":{"id":"hZkTWC8xM7fZ"},"id":"hZkTWC8xM7fZ"},{"cell_type":"markdown","source":["쿠팡의 검색결과 URL은 <br>'https://www.coupang.com/np/search?q={keyword}&channel=user&sorter=scoreDesc&listSize=36&isPriceRange=false&rating=0&page={page}&rocketAll=false'<br>\n","이런 구조로 구성돼 있다.\n","\n","검색의 키워드와 페이지 번호만 있으면 해당하는 상품들을 확인할 수 있다.\n","\n","이번 과제에서는 세제를 검색했을 때, 1페이지부터 끝페이지까지 있는 상품의 이미지와 페이지 번호를 수집할 필요가 있다.\n","\n","이를 위해서 우선 쿠팡의 페이지 구조를 이해할 필요가 있다."],"metadata":{"id":"4mPDNbbnNzbI"},"id":"4mPDNbbnNzbI"},{"cell_type":"markdown","source":["쿠팡의 경우에 키워드로 검색하여 노출되는 페이지가 최대 27페이지까지 있다.<br>\n","인기가 없어서 등록된 상품의 개수가 적은 키워드의 경우에는 끝페이지가 27페이지가 아닐수도 있다.<br>\n","그래서 쿠팡의 HTML 구조에서 끝페이지의 정보를 담고 있는 태그를 추출하는 함수를 작성했다."],"metadata":{"id":"kgb6Kz9uNR2S"},"id":"kgb6Kz9uNR2S"},{"cell_type":"markdown","source":["구글 크롬에서 F12를 눌러 개발자 도구를 이용하면 웹페이지에서 원하는 정보가 HTML 구조 어디에 위치한지 쉽게 찾을 수 있다."],"metadata":{"id":"cpoWakOSPqv5"},"id":"cpoWakOSPqv5"},{"cell_type":"code","execution_count":null,"id":"6b991364","metadata":{"id":"6b991364"},"outputs":[],"source":["# 검색된 키워드의 마지막 페이지 번호를 추출하는  함수\n","\n","def find_last_page(keyword):\n","    baseurl = 'https://www.coupang.com'\n","    headers = {\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36' # 크롤러가 아니라 사람이라고 하는 부분\n","    }\n","\n","    first_url = f'https://www.coupang.com/np/search?q={keyword}&channel=user&sorter=scoreDesc&listSize=36&isPriceRange=false&rating=0&page=1&rocketAll=false'\n","    response = requests.get(first_url, headers=headers)\n","    soup = BeautifulSoup(response.content, \"lxml\") #lxml을 사용해서 soup에 넣겠다.\n","    products_page = soup.find('a', class_='btn-last disabled').text # soup에서 a tag에서 class=btn-last disabled인것을 찾아라\n","    \n","    return int(products_page) # 키워드의 조회가능한 마지막 페이지 번호 반환"]},{"cell_type":"code","execution_count":null,"id":"c25d43e9","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":457},"id":"c25d43e9","executionInfo":{"status":"error","timestamp":1653021290589,"user_tz":-540,"elapsed":61872,"user":{"displayName":"황상민경제학과","userId":"16202137338056398609"}},"outputId":"79b4aefe-b53d-4954-c92f-8f61fd345b9f"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-4cf0fa4302ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfind_last_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'세제'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 쿠팡에서 세제를 검색했을 때 조회할 수 있는 끝페이지\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-dcd34ed87285>\u001b[0m in \u001b[0;36mfind_last_page\u001b[0;34m(keyword)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfirst_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'https://www.coupang.com/np/search?q={keyword}&channel=user&sorter=scoreDesc&listSize=36&isPriceRange=false&rating=0&page=1&rocketAll=false'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#lxml을 사용해서 soup에 넣겠다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mproducts_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'btn-last disabled'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;31m# soup에서 a tag에서 class=btn-last disabled인것을 찾아라\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["find_last_page('세제') # 쿠팡에서 세제를 검색했을 때 조회할 수 있는 끝페이지"]},{"cell_type":"markdown","source":["## 2.2. 추출할 상품의 데이터 수집"],"metadata":{"id":"2j52WwrCZbSy"},"id":"2j52WwrCZbSy"},{"cell_type":"markdown","id":"c8d31cfd","metadata":{"id":"c8d31cfd"},"source":["code source: https://goodthings4me.tistory.com/562"]},{"cell_type":"code","execution_count":null,"id":"64949a92","metadata":{"id":"64949a92"},"outputs":[],"source":["# code source: https://goodthings4me.tistory.com/562\n","# 긁어올 상품들의 정보 추출\n","\n","def coupang_products(keyword):\n","    pages = find_last_page(keyword)-1 # 끝페이지는 비어있어서 크롤링 오류가 나는 경우가 있으므로, 뒤에서 두번재 페이지까지 사용한다.\n","    \n","    baseurl = 'https://www.coupang.com'\n","    headers = {\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36'\n","    }\n","\n","    products_link = []\n","    file_number = 0\n","    for page in range(1, pages + 1): # 1 페이지부터 pages(맨 마지막에서 앞의 페이지)까지 있는 매물 다 긁어\n","        url =f'https://www.coupang.com/np/search?q={keyword}&channel=user&sorter=scoreDesc&listSize=36&isPriceRange=false&rating=0&page={page}&rocketAll=false'\n","        response = requests.get(url, headers=headers)\n","        soup = BeautifulSoup(response.content, \"lxml\") #lxml을 사용해서 soup에 넣겠다\n","        products_lis = soup.find('ul', id='productList').find_all('li') # soup에서 ul tag에서 id=productList인거에서li'태그를 모두 가져온다.\n","        for li in products_lis: # 페이지 내에 있는 모든 product list를 확인하면서\n","            a_link = li.find('a', href=True)['href'] # li tag에서 a태그를 찾아서 href가 있는 tag를 찾아서 href를 a_link에 넣어라 \n","            prd_link = baseurl + a_link # base url이랑 a_link 이어붙여서 prd_link에 넣어라\n","            prd_name = li.find('div', class_='name').text # li tag에서 div tag가 class=name인 것을 텍스트로 변환해서 제품명으로 저장하라\n","            prd_page = page # 현재 긁고 있는 페이지를 상품 페이지에 저장하라\n","            try: # try code가 되면 try code를 실행하고\n","                base_price = li.find('span', class_='price-info').find('del', class_='base-price').text\n","            except: # try code가 실행되지 않으면 except 코드를 실행해라\n","                base_price = ''\n","            price = li.find('strong', class_='price-value').text\n","\n","            file_number = file_number + 1 # 페이지 번호에 상관없이 수집하고 있는 파일에 임의로 부여하는 숫자\n","            file_number_str = str(file_number).zfill(6) # 임의로 부여된 숫자를 6자리 고정\n","            products_info = {\n","                'file_name': 'coupang_' + '%dpage_' %page + '%s_' %keyword + file_number_str + '.jpg', \n","                'product_type': keyword,\n","                'product_name': prd_name,\n","                'base_price': base_price,\n","                'price': price,\n","                'product_page': prd_page,\n","                'product_url': prd_link\n","            }\n","            products_link.append(products_info)\n","\n","            time.sleep(2)\n","        time.sleep(5)\n","\n","    print(len(products_link))\n","    df = pd.DataFrame(products_link)\n","    print(df)\n","    df.to_csv('coupang_%s_%dpages_list.csv' %(keyword, pages), index=False, encoding='utf-8-sig')"]},{"cell_type":"code","execution_count":null,"id":"d7d7d673","metadata":{"id":"d7d7d673"},"outputs":[],"source":["start = time.time() \n","\n","coupang_products('세제')\n","\n","end = time.time() \n","print(f\"{end - start:.5f} sec\")"]},{"cell_type":"markdown","source":["## 2.3. 추출한 상품 데이터의 링크를 통해 이미지 추출 및 저장"],"metadata":{"id":"ZidIFwiiZvDn"},"id":"ZidIFwiiZvDn"},{"cell_type":"markdown","id":"9a4cf147","metadata":{"id":"9a4cf147"},"source":["code source: https://data-make.tistory.com/170"]},{"cell_type":"code","execution_count":null,"id":"27f940f8","metadata":{"id":"27f940f8"},"outputs":[],"source":["# 추출한 이미지를 저장할 폴더를 생성하는 함수\n","\n","def createFolder(directory):\n","    import os\n","    try:\n","        if not os.path.exists(directory):\n","            os.makedirs(directory)\n","    except OSError:\n","        print ('Error: Creating directory. ' +  directory)"]},{"cell_type":"code","execution_count":null,"id":"43fcb147","metadata":{"id":"43fcb147"},"outputs":[],"source":["# 이미지 크롤링\n","\n","def crawl_img_from_df(input_df) :   \n","    for index in range(0, len(input_df)) :       \n","        img_url = input_df.iloc[index,:]['product_url']\n","        headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"}\n","        res = requests.get(img_url, headers=headers)\n","        res.raise_for_status() # 200 OK 코드가 아닌 경우 에러 발동\n","        soup = BeautifulSoup(res.text, \"lxml\") \n","        \n","        prd_img_url = \"http:\"+ soup.select_one('#repImageContainer > img')['src']\n","        prd_type = input_df.iloc[index,:]['product_type']\n","        prd_page = input_df.iloc[index,:]['product_page']\n","        img_name = input_df.iloc[index,:]['file_name']\n","        img_path1 = './%s_img/'%(prd_type)\n","        img_path2 = './%s_%dpage/'%(prd_type, prd_page)\n","        \n","        # 클래스별로 저장필요할 때 사용\n","        #createFolder('%s_%dpage' %(prd_type, prd_page))\n","        #urllib.request.urlretrieve(prd_img_url, img_path2 + \"%s\" %(img_name))\n","        \n","        # 그냥 한 폴더에 이미지 저장\n","        createFolder('%s_img' %(prd_type))\n","        urllib.request.urlretrieve(prd_img_url, img_path1 + \"%s\" %(img_name))\n","        time.sleep(5)"]},{"cell_type":"code","execution_count":null,"id":"71fb937d","metadata":{"id":"71fb937d"},"outputs":[],"source":["start = time.time() \n","\n","df_detergent = pd.read_csv('coupang_세제_26pages_list.csv')\n","crawl_img_from_df(df_detergent)\n","\n","# 이미지 저장에 얼마나 걸린지 확인\n","end = time.time() \n","print(f\"{end - start:.5f} sec\")"]},{"cell_type":"markdown","source":["Part1을 통해 수집한 상품 정보 및 이미지 데이터는 연구자가 직접 Dropbox로 옮겨 Part2에서 누구나 접근가능하도록 제공함"],"metadata":{"id":"kD3D9ZubkT2G"},"id":"kD3D9ZubkT2G"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"과제2_Part1_120220243_황상민.ipynb","provenance":[{"file_id":"1m1_Sj7yL7RQOVIaLeGnGLJ09g0WtVuxa","timestamp":1649475368677}],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}